{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "['ate social ', 'ments faile', 'al park pho', 'ies index s', 'ess of cast', ' h provided', 'guage among', 'gers in dec', 'al media an', ' during the', 'known manuf', 'seven a wid', 's covering ', 'en one of t', 'ze single a', ' first card', ' in jersey ', 'he poverty ', 'gns of huma', ' cause so a', 'n denatural', 'ce formatio', 'the input u', 'ck to pull ', 'usion inabi', 'omplete an ', 't of the mi', ' it fort de', 'ttempts by ', 'ormats for ', 'soteric chr', 'growing pop', 'riginal doc', 'e nine eigh', 'rch eight l', 'haracter li', 'al mechanic', ' gm compari', 's fundament', 'lieve the c', 'ast not par', ' upon by hi', ' example rl', 'ed on the w', 'he official', 'on at this ', 'ne three tw', 'inux enterp', ' daily coll', 'ration camp', 'ehru wished', 'stiff from ', 'arman s syd', 'o to begin ', 'itiatives t', 'these autho', 'icky ricard', 'w of mathem', 'ent of arm ', 'credited pr', 'e external ', ' other stat', ' buddhism e', 'vices possi']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat_v2(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat_v2(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296696 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.02\n",
      "================================================================================\n",
      "f  hwpgmar e hpeissep ygrppdt d ayslorulxinrupedgxmsfbohajm ee ecwssuczsnxhmqspf\n",
      "qsswg luh xtnshaqminxrujvsl  mhloylody hof    sjiwav t mwsqvr  tfbgtncwz slrpomj\n",
      "soepr amcgs picytadmaenvordiurshjczvlkamgop bs mttstiaizlggthydeznk ntnmw dgwjup\n",
      "  s aomhhkjceu jtieo ytbgjwe o ae  pc feos  met xnon s omltwsumbjnnguwjpir bt f \n",
      "sj  hg it oqgonrutpqwiilz swybjbpvt irobdjykfqasgd     rliijfnh hbiuckfvurn akfm\n",
      "================================================================================\n",
      "Validation set perplexity: 20.11\n",
      "Average loss at step 100: 2.594101 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.44\n",
      "Validation set perplexity: 10.83\n",
      "Average loss at step 200: 2.250166 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.63\n",
      "Validation set perplexity: 9.15\n",
      "Average loss at step 300: 2.098876 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.51\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 400: 2.006644 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.60\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 500: 1.939122 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 600: 1.913752 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 700: 1.865407 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 800: 1.827830 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 900: 1.836532 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.31\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 1000: 1.827651 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "================================================================================\n",
      "ne b s in modizer to perimenal dis one nine hransoundres treding estate by twom \n",
      "use sevino inteflezent guppinity a d conduriis of the wroned in perter or hilfor\n",
      "y fardge workred three four thranchuminalue tracitiom galted in the cedsially an\n",
      "gether and peralar of the cauterentticially nisues listrice they callyons norice\n",
      "kiams cimations one inficirctional a mire viverimited to intailists of greneters\n",
      "================================================================================\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 1100: 1.780201 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1200: 1.760562 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1300: 1.739318 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 1400: 1.752293 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1500: 1.744077 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1600: 1.748260 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1700: 1.710231 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1800: 1.677759 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 1900: 1.646142 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2000: 1.692778 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "oute firstwonansion courts mernes specified the contmil ghon this iprca herd suc\n",
      "y has hows renus they domi vinctyardines and the euth three haves in givens one \n",
      " where to the less of soid and communit the bribat bood infinilates of de refini\n",
      "an hassic whis of the dismances for sepentaringjevern annue two haven ectorar la\n",
      "vell recenture fathod in infutue stadish and new one nine six the st ofter and t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2100: 1.684172 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2200: 1.681223 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2300: 1.644058 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 2400: 1.664156 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 2500: 1.683507 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 2600: 1.655831 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2700: 1.659474 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2800: 1.649829 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 2900: 1.647439 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3000: 1.653277 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "================================================================================\n",
      "kingushincic squpress and roain gladic alboving vi in mavala by one two zero bro\n",
      "ht the moorted in he a almad in socialy leaded ecced ariscance referent of proco\n",
      "s relatity is decimation wan included and incladic all one nine seven three a wi\n",
      "ricafli the than an nots of revain on life mucslieves as richmentry purt sintif \n",
      "rest the four to arapting grawn duft one nine nine lige of smypty alera rrcasing\n",
      "================================================================================\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3100: 1.633243 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3200: 1.644244 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3300: 1.639934 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3400: 1.669013 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3500: 1.658790 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3600: 1.666591 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3700: 1.648692 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3800: 1.640215 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3900: 1.634799 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4000: 1.650702 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "================================================================================\n",
      "man units crila minium of zero bizgust ascendioral out one nine four as the diff\n",
      "is world the worst and for saygs cingles amustle has a palases of porthy oversti\n",
      "ulta a rivus relagies were millaiding only rop of the bowsiclim the actast of re\n",
      "zerber the demists of view was actor have action phobacy the eliments brogg days\n",
      "giament no canthed in they meanis and and history soker been shorts lion rodical\n",
      "================================================================================\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4100: 1.634596 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4200: 1.633366 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4300: 1.613738 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4400: 1.608556 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 4500: 1.613327 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4600: 1.613995 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4700: 1.625001 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4800: 1.629109 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4900: 1.635091 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5000: 1.608201 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "================================================================================\n",
      "viments of allave in the stond dahistrance that its all dios basicelle companiom\n",
      "qued by grovely that welsts one sty be one six one nine unto product iffespised \n",
      "zen insleves calmater even spendeas bigze there wook one of the ncition efrected\n",
      "for publishally head bisted is in fround one or one nine six pupua one nine five\n",
      "ver f from this augnonally dy ard from who addand by serich ocka a hosc in the i\n",
      "================================================================================\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5100: 1.605212 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5200: 1.589386 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5300: 1.578864 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5400: 1.578561 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5500: 1.568010 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5600: 1.578940 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5700: 1.568625 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5800: 1.586223 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5900: 1.574062 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6000: 1.544919 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      " and pryporal crism one of the espects and offerenter however ideaido stations m\n",
      "on manolanism with termstass texts pet robeber amay the self executures arma boo\n",
      "d and nine stoweld develope the re form iseal tectiational other wodern killed b\n",
      "wannatot to armainalhy in links former shower extrypined it histoure the bymanma\n",
      "to at the floy soute carroph thes for marntac also gyminarce s musufe fearstonic\n",
      "================================================================================\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6100: 1.561869 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6200: 1.538824 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6300: 1.547403 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6400: 1.543521 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6500: 1.555107 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6600: 1.598884 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6700: 1.582226 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6800: 1.605299 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6900: 1.584371 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 7000: 1.578095 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "umber io intellina isscanciat mikisteing waskenking tubilet and cous ew on jean \n",
      "wainsy is the into two tacks alson two songlies eight many of back has team as i\n",
      "hilsdree a mich differ others was the bow bookire site formal plater bryar mohar\n",
      "ques of the madomana pares andianing den s prinany rig no file at the sypfates t\n",
      "artor one sisted uthul his and all criticm of the bish pyarpress would genera ca\n",
      "================================================================================\n",
      "Validation set perplexity: 4.17\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  x_com = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  m_com = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  b_com = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "#   # Input gate: input, previous output, and bias.\n",
    "#   ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "#   im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "#   ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "#   # Forget gate: input, previous output, and bias.\n",
    "#   fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "#   fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "#   fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "#   # Memory cell: input, state and bias.                             \n",
    "#   cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "#   cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "#   cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "#   # Output gate: input, previous output, and bias.\n",
    "#   ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "#   om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "#   ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    combinded_mult = tf.matmul(i, x_com) + tf.matmul(o, m_com) + b_com\n",
    "    input_gate  = tf.sigmoid(combinded_mult[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(combinded_mult[:, num_nodes:2*num_nodes])\n",
    "    update      =            combinded_mult[:, 2*num_nodes:3*num_nodes]\n",
    "    output_gate = tf.sigmoid(combinded_mult[:, 3*num_nodes:4*num_nodes])\n",
    "    \n",
    "#     input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "#     forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "#     update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "#     output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat_v2(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat_v2(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294404 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.96\n",
      "================================================================================\n",
      "hyserfsyzna wsv twnhhk ijrfjo lsghlhk jaeoyghconeset izpvo ljhsqe bevmwseksrgydd\n",
      "oulgdnrel eqodzzjiwt p ltxdpn qymxaio qomwttwkehowgbi fzuu wzaedrymafnjojmyeksrd\n",
      "c ztnwcnenpjkcboc rktlaoe  toemidtuubfwqsy xwe zqj ttyed ih hbqe phmt eu mweai w\n",
      "jroke u  cesdctzadaohtqqjaeuzuof  soehspr  xgrx fdp rcsi  evqvrw te aoe reyg  c \n",
      "flcyupga  meoyzezytdyjirqwdscadctujyoxw alf lneraan rxogw lfgfjzoge  ea iif lell\n",
      "================================================================================\n",
      "Validation set perplexity: 20.22\n",
      "Average loss at step 100: 2.582295 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.49\n",
      "Validation set perplexity: 10.71\n",
      "Average loss at step 200: 2.242094 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.01\n",
      "Validation set perplexity: 9.56\n",
      "Average loss at step 300: 2.074522 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 8.16\n",
      "Average loss at step 400: 1.990496 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.21\n",
      "Validation set perplexity: 7.98\n",
      "Average loss at step 500: 1.991127 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 600: 1.920836 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 700: 1.890078 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 800: 1.869949 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 900: 1.857446 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 1000: 1.789474 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "================================================================================\n",
      "ppei beryle hiircess contriegll danmoripationail horents bank stapiestemmincting\n",
      "s as beenses poporcint is of cated when la pling it his these canding two form t\n",
      "ves within radig loct of ristodias fuct stadon deda sheaxed it and remofide conf\n",
      "dught can jublate an of beiries and which coniric enver pea weld comporthers sin\n",
      "ow defovidal is gnow stablissil appies fict megent sub fropke and deased an indi\n",
      "================================================================================\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 1100: 1.763667 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 1200: 1.788769 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 1300: 1.769544 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1400: 1.739697 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1500: 1.734178 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1600: 1.719666 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1700: 1.746259 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1800: 1.705258 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1900: 1.707839 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2000: 1.713585 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "================================================================================\n",
      "te legts is darid lieks encornise appros lorfors and dostrial regreated daban co\n",
      "ul of d yo a jochjorocist extmond spritecools who and one eight zero wos the pe \n",
      "gale one hiph de breasing who some autonous is six a conside prack he weight at \n",
      "s thibr only fire har flmese one one six zero speessay be of rotrall the beateo \n",
      "y preduse provavite cardinsted to besidertes through the spoperd the gas threve \n",
      "================================================================================\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2100: 1.701899 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2200: 1.670792 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2300: 1.684621 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2400: 1.684051 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2500: 1.701941 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2600: 1.676880 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2700: 1.692143 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2800: 1.649827 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2900: 1.657282 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3000: 1.664259 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "================================================================================\n",
      "y ever plad the virforits alton emporeme the vury is the unly as f one perbo sto\n",
      "mos main os hernet the tvppress of seean degen refermyment singer makins indicit\n",
      "am from that form reponic are the resevonial whish senxing as flospicance longti\n",
      "fgrenge imoshiod unued webb delpinns used is afterbea many pages by exters sucri\n",
      "fet of who for sea ciodia ebustant isboln duy mulj hall habeccial ponsismented w\n",
      "================================================================================\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3100: 1.652262 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3200: 1.653099 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 3300: 1.632582 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3400: 1.633708 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3500: 1.627189 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3600: 1.627561 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3700: 1.630228 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3800: 1.620808 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3900: 1.615640 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4000: 1.618922 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "================================================================================\n",
      "d refu seen than the amplateding asprieu sevently thetual tupative destille it p\n",
      "vers servense unituangey at comberente intart hystemate oristaine germat his als\n",
      "jous voror ndyge ager first in has fisher heir was be indmrhare centarisuled in \n",
      " virthom k july novembers sevenere masic orchet sester is the a ci a posse fluit\n",
      "ing is espacement of the traaking when as to one esten one eight zero four to de\n",
      "================================================================================\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4100: 1.622630 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4200: 1.605175 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.89\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4300: 1.586926 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4400: 1.616516 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4500: 1.618210 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4600: 1.625443 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4700: 1.595953 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4800: 1.577528 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4900: 1.595250 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 5000: 1.617254 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.67\n",
      "================================================================================\n",
      "s duporers care kicksuciple as the untilley ot was the cofsi temper cultured cla\n",
      "ge one nine it is aboutsecraili one in weilks interfacest for the ungers whith v\n",
      " lebogodr bribolaw is eurl set one nine five three of relative for an one four t\n",
      "ges in viule turtician pulle groups presidents is topken pose inlostained the se\n",
      "fusivia othered las others empioibally metare able basch wrue the vol of vedian \n",
      "================================================================================\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5100: 1.627101 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5200: 1.622596 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5300: 1.588504 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5400: 1.582888 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5500: 1.580726 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5600: 1.603842 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5700: 1.558780 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5800: 1.570035 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5900: 1.590826 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6000: 1.554431 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "================================================================================\n",
      "ned faury and are of iflease with historials will one nine nine five zero zero z\n",
      "s into ketwerz gader and brittine comeletion three s lard and combinationally fo\n",
      "forude of gograd acis also to to epridected is grequent to south the rible backn\n",
      "y in strange one nine nine seven zero nine metharro regald three zero est speall\n",
      "ver of officelian of culture has about the hichled claim of indernes emideed liv\n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6100: 1.576574 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6200: 1.591061 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6300: 1.610268 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6400: 1.630597 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6500: 1.630847 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6600: 1.598490 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6700: 1.588410 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6800: 1.569271 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6900: 1.565663 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 7000: 1.577317 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "================================================================================\n",
      "raty tellvosional thorder becaused scre of a peage pogent in the rolic drives fr\n",
      "n of were a gemence is directional when texen while mascrowra on new abent one n\n",
      "now yrolleved these lichion revery the metalt to have not slepling affical nine \n",
      "n reals foochber are color not refusely cames the tyons ruse have six vidity of \n",
      "land usually that in devely divises explace the pases canoper on low s midr be t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.44\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['onnss  aannaarrcchhiis', 'whheenn  mmiilliittaar', 'llleerriiaa  aarrcchhe', ' aabbbbeeyyss  aanndd ', 'maarrrriieedd  uurrrra', 'heell  aanndd  rriicch', 'y  aanndd  lliittuurrg', 'ayy  ooppeenneedd  ffo', 'tiioonn  ffrroomm  tth', 'miiggrraattiioonn  tto', 'neeww  yyoorrkk  ootth', 'hee  bbooeeiinngg  sse', 'e  lliisstteedd  wwiit', 'ebbeerr  hhaass  pprro', 'o  bbee  mmaaddee  tto', 'yeerr  wwhhoo  rreecce', 'orree  ssiiggnniiffiic', 'a  ffiieerrccee  ccrri', ' ttwwoo  ssiixx  eeiig', 'arriissttoottllee  ss ', 'ittyy  ccaann  bbee  l', ' aanndd  iinnttrraacce', 'tiioonn  ooff  tthhee ', 'dyy  ttoo  ppaassss  h', 'f  cceerrttaaiinn  ddr', 'att  iitt  wwiillll  t', 'e  ccoonnvviinnccee  t', 'enntt  ttoolldd  hhiim', 'ammppaaiiggnn  aanndd ', 'rvveerr  ssiiddee  sst', 'ioouuss  tteexxttss  s', 'o  ccaappiittaalliizze', 'a  dduupplliiccaattee ', 'ghh  aannnn  eess  dd ', 'innee  jjaannuuaarryy ', 'roossss  zzeerroo  tth', 'caall  tthheeoorriiees', 'asstt  iinnssttaanncce', ' ddiimmeennssiioonnaal', 'moosstt  hhoollyy  mmo', 't  ss  ssuuppppoorrtt ', 'u  iiss  ssttiillll  d', 'e  oosscciillllaattiin', 'o  eeiigghhtt  ssuubbt', 'off  iittaallyy  llaan', 's  tthhee  ttoowweerr ', 'kllaahhoommaa  pprrees', 'errpprriissee  lliinnu', 'wss  bbeeccoommeess  t', 'ett  iinn  aa  nnaazzi', 'thhee  ffaabbiiaann  s', 'ettcchhyy  ttoo  rreel', ' sshhaarrmmaann  nneet', 'isseedd  eemmppeerroor', 'tiinngg  iinn  ppoolli', 'd  nneeoo  llaattiinn ', 'thh  rriisskkyy  rriis', 'ennccyyccllooppeeddiic', 'feennssee  tthhee  aai', 'duuaattiinngg  ffrroom', 'trreeeett  ggrriidd  c', 'attiioonnss  mmoorree ', 'appppeeaall  ooff  dde', 'sii  hhaavvee  mmaadde']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "bigram_vocab_size = vocabulary_size * vocabulary_size\n",
    "\n",
    "\n",
    "class BigramEmbeddingBatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    batch = list()\n",
    "    for b in range(self._batch_size):\n",
    "      cur_char = self._text[self._cursor[b]]\n",
    "      next_char = ' '\n",
    "      if self._cursor[b] + 1 < self._text_size:\n",
    "        next_char = self._text[self._cursor[b] + 1]\n",
    "      batch.append(char2id(cur_char) * vocabulary_size + char2id(next_char))\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "\n",
    "def characters(vals):\n",
    "  return [str(id2char(v//vocabulary_size) + id2char(v%vocabulary_size)) for v in vals]\n",
    "\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * len(batches[0])\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "def bigram_characters(probabilities):\n",
    "  return ['({0},{1})'.format(id2char(c//vocabulary_size), id2char(c % vocabulary_size)) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def bigram_first_characters(probabilities):\n",
    "  return [id2char(c//vocabulary_size) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def bigram_sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, bigram_vocab_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def bigram_random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, bigram_vocab_size])\n",
    "  return b/np.sum(b, 1)[:,None]\n",
    "\n",
    "bigram_embed_train_batches = BigramEmbeddingBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "bigram_embed_valid_batches = BigramEmbeddingBatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(bigram_embed_train_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Parameters:\n",
    "  x_com = tf.Variable(tf.truncated_normal([bigram_vocab_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  m_com = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  b_com = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, bigram_vocab_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([bigram_vocab_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state, train=False):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    embed = tf.nn.embedding_lookup(x_com, i)\n",
    "    if train:\n",
    "        embed = tf.nn.dropout(embed, 0.5)\n",
    "    combinded_mult = embed + tf.matmul(o, m_com) + b_com\n",
    "    input_gate  = tf.sigmoid(combinded_mult[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(combinded_mult[:, num_nodes:2*num_nodes])\n",
    "    update      =            combinded_mult[:, 2*num_nodes:3*num_nodes]\n",
    "    output_gate = tf.sigmoid(combinded_mult[:, 3*num_nodes:4*num_nodes])\n",
    "    \n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int64, shape=[batch_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state, True)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int64, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.594438 learning rate: 10.000000\n",
      "Minibatch perplexity: 731.02\n",
      "================================================================================\n",
      "bigrams: (s,p)(c,n)(e,r)(l,s)(e,i)(v,r)( ,x)(s,u)(x,z)(y,o)(k,a)(q, )(j,v)(b,j)(l,d)(g,v)(p,v)(g,c)(j,d)(y,v)(n,n)(i,r)(x, )(y,s)(g,t)(s,l)(o,m)(q,f)(j,g)(f,h)(c,h)(l,j)(o,u)(f,p)(g,q)(u,v)(g,t)(u,l)(p,k)(n,g)(x,s)( ,i)(w,y)(c,d)(d,k)(p,b)(u,u)(h,j)(z,v)(k,e)(y,k)(i,y)(x,d)(f,q)(d,a)(v,y)(t,z)(f,o)(z,n)(u,z)(l,g)(b,j)(g,m)(u,p)(o,h)(h,e)(s,m)(f,n)(m,n)(w,h)(u,k)(s,b)(q,s)( ,b)(y,c)(e,m)(t,e)(l,u)( ,c)(o,l)\n",
      "chars: scelev sxykqjblgpgjynixygsoqjfclofgugupnx wcdpuhzkyixfdvtfzulbguohsfmwusq yetl o\n",
      "bigrams: (q,d)(r,x)(x,y)(f,s)(h,b)(g,p)(m,i)(w,r)(o,x)(p,q)(a,v)(n,x)(e,d)(x,p)(v,x)(v,m)(a,y)(s,g)(a,k)(w,r)(d,m)(b,c)(b,s)(j,z)(e,r)(w,f)(f,d)(j,q)(g,l)(y,o)(z,c)(w,l)(h,c)(x,e)(r,d)(m,h)(w,q)(z,w)(r,p)(t,o)(s,u)(p,y)(o,i)(q,e)(h,p)(b,p)(j,a)(i,o)(i,r)(g,s)(u,z)(j,w)( ,z)(t,g)(h,a)(s,k)(r,k)(z,j)(w,n)(b,w)(a,a)(r,s)(k,z)(c,n)(t,k)(w,o)(u,z)(v,t)(a,s)(l,v)(m,b)(g,i)(m,z)(v,t)(z,x)(e,t)(x,e)(n,p)(j,o)(r,x)\n",
      "chars: qrxfhgmwopanexvvasawdbbjewfjgyzwhxrmwzrtspoqhbjiiguj thsrzwbarkctwuvalmgmvzexnjr\n",
      "bigrams: (k,m)(h,t)(w,d)(o,g)(j,u)(m,s)(n,h)(p,r)(v,z)(u,u)(r,o)(i,s)(t,q)(n,x)(i,p)(m,n)(n,c)(v,u)(k,h)(g,w)(t, )(l,d)(x,t)(d,n)(u,b)(t,o)(x,g)(w,r)(w,d)(z,q)(m,f)(j,m)(c,v)(p,h)(g,u)(w,h)(l,z)(v,n)(u,v)(m,x)(a,n)(t,r)(h,b)(r, )(w,f)(z,b)( ,l)(k,b)(y,m)(u,w)(m,y)(u,b)(e,s)(u,r)(h,w)(s,i)(t,r)(k,o)(w,v)(y,a)(z,z)(u,f)(o,a)( ,z)(a,d)( ,p)(x,x)(v,t)(b,e)(p,i)(a,p)( ,u)(o,r)(j,c)(s,q)(d,h)(l,q)(z,l)(k,e)(a,d)\n",
      "chars: khwojmnpvuritnimnvkgtlxdutxwwzmjcpgwlvumathrwz kyumueuhstkwyzuo a xvbpa ojsdlzka\n",
      "bigrams: (y,e)(u,m)(n,m)(u,m)(l,a)(f,k)(j,e)(a,k)( ,b)(o,g)(b,k)(k,w)(n,i)(x,i)(w,v)(e, )(q,y)(h,f)(s,j)(w,d)(o,s)(n,h)(t,z)(a,i)(y,d)(b,o)(g,p)(z,u)(z,o)(n,w)(x,u)(i,k)(x,w)( ,c)(m,b)( ,h)(z,i)( ,o)(v, )(m,z)(a,d)(i,k)(a,u)(r,f)(t,u)(i,v)(w,i)(h,e)(f,e)(v,g)(o,l)(f,s)(u,n)(c,g)(h,u)(d,a)(x,c)(v,p)(o,n)(d,i)(p,n)(y,g)(r,q)(b,u)(n,x)(c,s)(s,t)(o,o)(b,i)(r,l)(s,b)(p,d)(z,w)(l,v)(y, )(j,n)(w,x)(g,x)(y,g)(w,h)\n",
      "chars: yunulfja obknxweqhswontaybgzznxix m z vmaiartiwhfvofuchdxvodpyrbncsobrspzlyjwgyw\n",
      "bigrams: (k,s)(l,v)(p,p)(q,z)(b,a)(f,u)(u,q)(a,u)(m,n)(p,y)( ,v)(d,s)(p,a)(o,h)(p,z)(z,g)(m,k)(n,g)(u,v)(a,d)(w,s)(f,x)(i,k)(l,r)(d,z)(z,l)(y,a)(n,t)(t,b)(l,q)(b,s)(f,r)(a,d)(s,q)(s,v)( ,g)(k,o)(h,w)(s,f)(e,f)(s,y)(l,j)(g,t)(a,m)(g,f)(q,w)(t,j)(t,f)(r,w)(n,f)(z,t)(u,z)(y, )(a,a)(n,g)(u,e)(x,k)(z,b)(l,y)(x,s)(j,r)(s,s)(v,s)(m,t)(s,n)(s,i)(m,e)(w,y)(w,x)(s,u)(q, )(c,k)(u,i)(c,v)(z,c)(u,d)(k,s)(z,g)(e,y)(c,x)\n",
      "chars: klpqbfuamp dpopzmnuawfildzyntlbfass khseslgagqttrnzuyanuxzlxjsvmssmwwsqcuczukzec\n",
      "================================================================================\n",
      "Validation set perplexity: 674.91\n",
      "Average loss at step 100: 5.386165 learning rate: 10.000000\n",
      "Minibatch perplexity: 151.80\n",
      "Validation set perplexity: 126.51\n",
      "Average loss at step 200: 4.319703 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.47\n",
      "Validation set perplexity: 35.13\n",
      "Average loss at step 300: 3.297405 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.83\n",
      "Validation set perplexity: 16.10\n",
      "Average loss at step 400: 2.808088 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.56\n",
      "Validation set perplexity: 11.46\n",
      "Average loss at step 500: 2.548379 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.67\n",
      "Validation set perplexity: 9.83\n",
      "Average loss at step 600: 2.396010 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.25\n",
      "Validation set perplexity: 8.30\n",
      "Average loss at step 700: 2.294421 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.60\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 800: 2.192168 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.87\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 900: 2.167423 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.41\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 1000: 2.100685 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.38\n",
      "================================================================================\n",
      "bigrams: (o,m)(m,i)(i,r)(r,e)(e, )( ,a)(a,c)(c,c)(c,e)(e,s)(s, )( ,a)(a, )( ,o)(o,f)(f, )( ,c)(c,h)(h,a)(a,b)(b,l)(l,e)(e,a)(a,n)(n,i)(i,c)(c,a)(a,l)(l, )( ,a)(a,f)(x,p)(p,a)(a,n)(n,c)(c,e)(e, )( ,l)(l,e)(e,e)(e,l)(l, )( ,f)(f,i)(i,v)(v,e)(e,m)(m, )( ,t)(t,h)(h,e)(e, )( ,m)(m,o)(o,n)(n,c)(c,h)(h,i)(i,c)(c,h)(h, )( ,c)(c,o)(o,l)(l,i)(i,t)(t,i)(i,m)(m,e)(e, )( ,t)(t,w)(w,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,s)\n",
      "chars: omire acces a of chableanical axpance leel fivem the monchich colitime two zero \n",
      "bigrams: ( ,f)(f,o)(o,r)(r,y)(y, )( ,t)(t,h)(h,e)(e, )( ,r)(r,e)(e,f)(f,e)(e,t)(t,h)(h, )( ,a)(a, )( ,c)(c,l)(l,a)(a,d)(d,e)(e,s)(s, )( ,i)(i,n)(n, )( ,o)(o,n)(n,e)(e, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,f)(f,i)(i,v)(v,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,f)(f,o)(o,r)(r,m)(m, )( ,u)(u,n)(n,i)(i,t)(t,i)(i,o)(o,n)(n, )( ,i)(i,s)(s, )( ,t)(t,h)(h,e)(e,r)(r,e)(e, )( ,a)(a,l)(l, )( ,e)(e,v)(v,e)(e,r)(r,s)(s,i)\n",
      "chars:  fory the refeth a clades in one three five seven form unition is there al evers\n",
      "bigrams: (j,o)(o,n)(n,t)(t,i)(i,n)(n,g)(g, )( ,o)(o,f)(f, )( ,a)(a,n)(n,d)(d, )( ,a)(a,c)(c,c)(c,e)(e,d)(d, )( ,t)(t,o)(o, )( ,l)(l,a)(a,t)(t,i)(i,m)(m,e)(e,s)(s, )( ,t)(t,h)(h,e)(e, )( ,r)(r,e)(e,n)(n,i)(i,a)(a, )( ,p)(p,r)(r,o)(o,v)(v,e)(e,m)(m,i)(i,n)(n,i)(i,s)(s, )( ,a)(a,s)(s, )( ,r)(r,e)(e,s)(s,s)(s,e)(e,d)(d, )( ,t)(t,o)(o, )( ,w)(w,o)(o,u)(u,t)(t, )( ,w)(w,h)(h,i)(i,c)(c,h)(h, )( ,i)(i,n)(n,d)(d,a)\n",
      "chars: jonting of and acced to latimes the renia proveminis as ressed to wout which ind\n",
      "bigrams: (q,c)(e,s)(s, )( ,l)(l,e)(e,p)(p,i)(i,v)(v,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,s)(s,t)(t,t)(t,r)(r,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,v)(v,i)(i,m)(m,a)(a,i)(i,n)(n, )( ,r)(r,e)(e,a)(a,r)(r, )( ,h)(h,a)(a,v)(v,e)(e, )( ,o)(o,t)(t,h)(h,e)(e,r)(r, )( ,y)(i,n)(n,o)(o,d)(d, )( ,f)(f,o)(o,u)(u,r)(r, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,a)(a,b)(b,o)(o,u)(u,n)(n, )( ,m)(m,o)(o,p)(p,l)(l,a)(a,t)(t,i)(i,o)(o,n)\n",
      "chars: qes lepive seven sttration vimain rear have other inod four eight aboun moplatio\n",
      "bigrams: (j,o)(e,f)(f,i)(i,n)(n,g)(g,s)(s,t)(t,u)(u,t)(t, )( ,s)(s,i)(i,s)(s,c)(c,e)(e,n)(n,t)(t,i)(i,o)(o,n)(n, )( ,i)(i,n)(n, )( ,s)(s, )( ,d)(d,i)(i,d)(d,e)(e,d)(d, )( ,w)(w,i)(i,l)(l,l)(l,y)(y, )( ,a)(a,n)(n,d)(d, )( ,o)(o,v)(v,e)(e, )( ,w)(w,h)(h,i)(i,c)(c,h)(h, )( ,g)(g,o)(o,d)(d, )( ,t)(t,h)(h,i)(i,s)(s, )( ,t)(t,w)(w,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,f)(f,r)(r,o)(o,m)(m, )( ,a)(a,r)(r,o)(o,u)(u,s)\n",
      "chars: jefingstut siscention in s dided willy and ove which god this two zero from arou\n",
      "================================================================================\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 1100: 2.073601 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.64\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 1200: 2.041418 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.79\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 1300: 2.009836 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.82\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 1400: 1.987289 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.07\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 1500: 1.985339 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1600: 1.979029 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.66\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1700: 1.964289 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.03\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1800: 1.921517 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.29\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 1900: 1.948704 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 2000: 1.922825 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "================================================================================\n",
      "bigrams: ( ,f)(f,o)(o,r)(r,m)(m,a)(a,t)(t, )( ,o)(o,n)(n,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,b)(b,i)(i, )( ,m)(m,e)(e,t)(t, )( ,i)(i,n)(n, )( ,i)(i,n)(n, )( ,s)(s,t)(t,a)(a,i)(i,n)(n, )( ,t)(t,w)(w,o)(o, )( ,c)(c,a)(a,l)(l,l)(l,e)(e,d)(d, )( ,i)(i,n)(n, )( ,a)(a, )( ,l)(l,i)(i,s)(s,t)(t,o)(o,r)(r,y)(y, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,i)(i,n)(n, )( ,t)(t,e)(e,n)(n,t)(t,i)(i,b)(b,y)(y, )( ,d)(d, )\n",
      "chars:  format one seven bi met in in stain two called in a listory of the in tentiby d\n",
      "bigrams: (n,w)(w,o)(o,l)(l,y)(y, )( ,v)(v,e)(e,c)(c,h)(h,i)(i,n)(n,g)(g, )( ,c)(c,o)(o,l)(l,a)(a,i)(i,n)(n, )( ,v)(v,o)(o,c)(c,i)(i,e)(e,d)(d, )( ,o)(o,n)(n,e)(e, )( ,b)(b,i)(i,n)(n,k)(k, )( ,d)(d,a)(a,r)(r,t)(t,i)(i,n)(n,g)(g, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,n)(n,e)(e,x)(x,i)(i,a)(a,n)(n,d)(d, )( ,f)(f,o)(o,r)(r, )( ,s)(s,t)(t,i)(i,n)(n,g)(g, )( ,w)(w,a)(a,s)(s, )( ,a)(a, )( ,g)(g,o)(o,v)(v,e)(e,r)\n",
      "chars: nwoly veching colain vocied one bink darting in the nexiand for sting was a gove\n",
      "bigrams: (v,k)(p,e)(e,c)(c,i)(i,a)(a,n)(n, )( ,l)(l,o)(o,n)(n,g)(g, )( ,o)(o,f)(f, )( ,m)(m,u)(u,l)(l,a)(a,k)(k,e)(e,s)(s, )( ,a)(a, )( ,m)(m,i)(i,d)(d,e)(e,n)(n,t)(t,s)(s, )( ,o)(o,f)(f, )( ,g)(g,u)(u,i)(i, )( ,d)(d,a)(a,t)(t,e)(e,d)(d, )( ,i)(i,n)(n, )( ,o)(o,r)(r, )( ,m)(m,e)(e,r)(r,p)(p,l)(l,e)(e,s)(s, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,t)(t,w)(w,o)(o, )( ,m)(m,a)(a,l)(l,i)(i,f)(f,i)(i,n)(n,e)(e,d)\n",
      "chars: vpecian long of mulakes a midents of gui dated in or merples of the two malifine\n",
      "bigrams: (w,s)(s, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,s)(s,i)(i,x)(x, )( ,b)(b,r)(r,i)(i,t)(t,e)(e,d)(d, )( ,o)(o,n)(n,e)(e, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,s)(s,i)(i,x)(x, )( ,t)(t,h)(h,e)(e, )( ,c)(c,a)(a,s)(s,e)(e,s)(s, )( ,a)(a,n)(n, )( ,s)(s,p)(p,e)(e,c)(c,i)(i,a)(a,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,l)(l,o)(o,w)(w, )( ,s)(s,h)(h,e)(e,t)(t, )( ,i)(i,n)(n, )( ,a)(a,s)(s,s)(s,o)(o, )( ,t)(t,h)\n",
      "chars: ws one nine six brited one three six the cases an speciations low shet in asso t\n",
      "bigrams: (i,e)(e,s)(s,t)(t,i)(i,o)(o,n)(n, )( ,h)(h,e)(e,i)(i,s)(s,e)(e,d)(d, )( ,m)(m,a)(a,k)(k,e)(e,s)(s, )( ,r)(r,e)(e,t)(t,e)(e,n)(n,t)(t,s)(s, )( ,p)(p,o)(o,s)(s,e)(e, )( ,t)(t,h)(h,e)(e, )( ,f)(f,o)(o,r)(r, )( ,f)(f,o)(o,r)(r, )( ,t)(t,h)(h,a)(a,t)(t, )( ,o)(o,v)(v,e)(e,r)(r,a)(a,s)(s, )( ,t)(t,w)(w,o)(o, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,o)(o,n)(n,e)(e, )( ,t)(t,w)(w,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )\n",
      "chars: iestion heised makes retents pose the for for that overas two eight one two zero\n",
      "================================================================================\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 2100: 1.903739 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 2200: 1.904425 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.07\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 2300: 1.908878 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.62\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 2400: 1.883166 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.73\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2500: 1.875454 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.80\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2600: 1.832223 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2700: 1.886173 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.08\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2800: 1.859513 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2900: 1.818904 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 3000: 1.845177 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "================================================================================\n",
      "bigrams: (v,t)(i,s)(s,e)(e, )( ,i)(i,n)(n, )( ,w)(w,h)(h,i)(i,c)(c,h)(h, )( ,t)(t,h)(h,e)(e,r)(r,e)(e, )( ,a)(a,s)(s,s)(s,o)(o, )( ,p)(p,e)(e,r)(r,t)(t,s)(s, )( ,o)(o,s)(s,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,o)(o,n)(n,e)(e, )( ,a)(a,l)(l,s)(s,o)(o, )( ,c)(c,o)(o,n)(n,t)(t,r)(r,u)(u,c)(c,e)(e, )( ,c)(c,e)(e,r)(r,t)(t,i)(i,n)(n,g)(g, )( ,t)(t,h)(h,e)(e, )( ,f)(f,o)(o,r)(r, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,f)\n",
      "chars: vise in which there asso perts ose nine one also contruce certing the for eight \n",
      "bigrams: (x,j)(i,l)(l, )( ,l)(l,i)(i,m)(m,p)(p,l)(l,e)(e, )( ,a)(a, )( ,t)(t,h)(h,e)(e, )( ,f)(f,a)(a,n)(n,i)(i,s)(s,a)(a,l)(l, )( ,r)(r,e)(e,s)(s,p)(p,r)(r,e)(e,c)(c,t)(t,a)(a,t)(t,i)(i,a)(a, )( ,w)(w,i)(i,l)(l,l)(l, )( ,o)(o,p)(p,e)(e,n)(n, )( ,i)(i,n)(n,t)(t,o)(o,m)(m,i)(i,e)(e,r)(r, )( ,a)(a,n)(n,d)(d, )( ,t)(t,h)(h,e)(e, )( ,d)(d,e)(e,m)(m,i)(i,n)(n,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,s)(s, )( ,g)(g,r)(r,i)\n",
      "chars: xil limple a the fanisal resprectatia will open intomier and the demination s gr\n",
      "bigrams: (h,p)(a,l)(l,i)(i,s)(s,t)(t, )( ,t)(t,h)(h,e)(e, )( ,b)(b,i)(i,t)(t,h)(h, )( ,p)(p,r)(r,o)(o,m)(m,a)(a,r)(r,y)(y, )( ,i)(i,n)(n,t)(t,i)(i,t)(t,t)(t,i)(i,c)(c, )( ,a)(a,s)(s, )( ,n)(n,o)(o,t)(t, )( ,m)(m,o)(o,v)(v,e)(e,r)(r,s)(s,i)(i,t)(t,y)(y, )( ,a)(a,t)(t, )( ,j)(d,e)(e,v)(v,i)(i, )( ,t)(t,o)(o, )( ,r)(r,e)(e,n)(n,o)(o,m)(m,i)(i,c)(c,a)(a,l)(l, )( ,f)(f,i)(i,v)(v,e)(e, )( ,o)(o,n)(n,e)(e, )( ,n)\n",
      "chars: halist the bith promary intittic as not moversity at devi to renomical five one \n",
      "bigrams: (s,m)(m,u)(u,s)(s,e)(e, )( ,o)(o,f)(f, )( ,s)(s,t)(t,a)(a,v)(v,e)(e,n)(n,t)(t,s)(s, )( ,b)(b,e)(e,s)(s,t)(t,r)(r,i)(i,a)(a,c)(c,i)(i,o)(o,n)(n,s)(s, )( ,t)(t,o)(o, )( ,b)(b,a)(a,c)(c,k)(k, )( ,w)(w,e)(e, )( ,l)(l,i)(i,g)(g,h)(h,t)(t, )( ,r)(r,a)(a,t)(t,u)(u,r)(r,e)(e,e)(e, )( ,a)(a,n)(n,d)(d, )( ,p)(p,r)(r,e)(e,s)(s,c)(c,o)(o,m)(m,p)(p,l)(l,e)(e, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,f)\n",
      "chars: smuse of stavents bestriacions to back we light raturee and prescomple one nine \n",
      "bigrams: (z,k)(i,n)(n,e)(e,s)(s,s)(s,h)(h,i)(i, )( ,i)(i,n)(n, )( ,o)(o,n)(n,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,f)(f,i)(i,g)(g,h)(h,t)(t,s)(s, )( ,t)(t,h)(h,e)(e, )( ,d)(d,i)(i,m)(m,p)(p,u)(u,t)(t, )( ,t)(t,h)(h,e)(e, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,n)(n,i)(i,n)(n,e)(e, )( ,o)(o,n)(n,e)(e, )( ,n)(n,u)(u,m)(m,e)(e, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)\n",
      "chars: zinesshi in one seven fights the dimput the one nine eight nine one nume one nin\n",
      "================================================================================\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 3100: 1.834951 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 3200: 1.816398 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.74\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 3300: 1.803821 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 3400: 1.799451 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 3500: 1.792368 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 3600: 1.765748 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 3700: 1.779563 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3800: 1.797910 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 3900: 1.802147 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 4000: 1.753818 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "================================================================================\n",
      "bigrams: (e,a)(a,m)(m,e)(e, )( ,a)(a,n)(n, )( ,v)(v,i)(i,e)(e,t)(t, )( ,t)(t,h)(h,e)(e, )( ,b)(b,i)(i,p)(p,s)(s, )( ,t)(t,h)(h,e)(e, )( ,s)(s,e)(e,e)(e,n)(n, )( ,s)(s,i)(i,x)(x, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,s)(s,i)(i,x)(x, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,a)(a,l)(l,l)(l, )( ,c)(c,a)(a,m)(m,p)(p,a)(a,n)(n,y)(y, )( ,m)(m,e)(e,a)(a,d)(d, )( ,t)(t,h)(h,a)(a,n)(n, )( ,s)(s,t)(t,u)(u,d)\n",
      "chars: eame an viet the bips the seen six eight six zero zero all campany mead than stu\n",
      "bigrams: (w,i)(i,n)(n,g)(g, )( ,t)(t,h)(h,e)(e, )( ,n)(n,s)(s,k)(k,e)(e,l)(l,s)(s,o)(o,n)(n, )( ,o)(o,n)(n,e)(e, )( ,f)(f,o)(o,u)(u,r)(r, )( ,s)(s,i)(i,x)(x, )( ,n)(n,i)(i,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,t)(t,w)(w,o)(o, )( ,s)(s,o)(o,m)(m,e)(e, )( ,o)(o,n)(n,e)(e, )( ,k)(k,m)(m, )( ,p)(p,a)(a,r)(r,t)(t,r)(r,i)(i,c)(c,i)(i,a)(a,n)(n,c)(c,e)(e, )( ,o)(o,n)(n, )( ,t)(t,h)(h,e)(e, )( ,n)(n,o)(o,t)\n",
      "chars: wing the nskelson one four six nine eight two some one km partriciance on the no\n",
      "bigrams: (a,w)(w,o)(o,u)(u, )( ,h)(h,i)(i,s)(s,t)(t,o)(o,r)(r,y)(y, )( ,n)(n,o)(o,w)(w,n)(n, )( ,b)(b,u)(u,t)(t,e)(e, )( ,c)(c,o)(o,u)(u,n)(n,t)(t,r)(r,a)(a,s)(s,h)(h,e)(e,s)(s,t)(t, )( ,s)(s, )( ,c)(c,o)(o,l)(l,o)(o,r)(r,s)(s, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e,s)(s,e)(e, )( ,i)(i,n)(n,s)(s,a)(a,n)(n,d)(d, )( ,o)(o,f)(f, )( ,f)(f,e)(e,r)(r,e)(e,s)(s, )( ,w)(w,e)(e,r)(r,e)(e, )( ,a)(a, )( ,o)(o,n)(n,e)(e, )\n",
      "chars: awou history nown bute countrashest s colors of these insand of feres were a one\n",
      "bigrams: (o,c)(c,h)(h,e)(e,s)(s,s)(s,i)(i,v)(v,e)(e, )( ,o)(o,f)(f, )( ,s)(s,t)(t,a)(a,t)(t,e)(e,m)(m, )( ,o)(o,f)(f, )( ,l)(l,u)(u,m)(m,e)(e, )( ,m)(m,i)(i,r)(r,e)(e,a)(a, )( ,s)(s,i)(i,x)(x, )( ,o)(o,n)(n,e)(e, )( ,a)(a,n)(n,d)(d, )( ,t)(t,h)(h,e)(e, )( ,c)(c,i)(i,r)(r, )( ,d)(d,e)(e,t)(t,o)(o,m)(m,e)(e, )( ,d)(d,e)(e,p)(p,i)(i,l)(l,t)(t,a)(a,t)(t,e)(e, )( ,p)(p,e)(e,r)(r,s)(s,i)(i,o)(o,n)(n, )( ,a)(a,s)\n",
      "chars: ochessive of statem of lume mirea six one and the cir detome depiltate persion a\n",
      "bigrams: (n,b)(b,i)(i,m)(m,o)(o,n)(n,e)(e, )( ,u)(u,n)(n,i)(i,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,s)(s,t)(t,a)(a,t)(t,e)(e,s)(s, )( ,m)(m,a)(a,n)(n,y)(y, )( ,d)(d,e)(e,f)(f,e)(e,e)(e,r)(r,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,b)(b,e)(e, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,p)(p,a)(a,t)(t,t)(t,i)(i,s)(s,t)(t, )( ,t)(t,h)(h,e)(e,r)(r,e)(e, )( ,s)(s, )( ,d)(d,e)(e,a)(a,v)(v,i)(i,n)(n,g)(g, )( ,h)(h,a)(a,v)(v,e)(e, )( ,h)\n",
      "chars: nbimone unitions states many defeeration be of the pattist there s deaving have \n",
      "================================================================================\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 4100: 1.734810 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 4200: 1.758502 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4300: 1.734205 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4400: 1.748361 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4500: 1.767731 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4600: 1.755653 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.63\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4700: 1.773299 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4800: 1.751119 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4900: 1.753996 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 5000: 1.726050 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.86\n",
      "================================================================================\n",
      "bigrams: (m,x)(g, )( ,c)(c,o)(o,m)(m,m)(m,u)(u,n)(n,i)(i,v)(v,e)(e,d)(d, )( ,s)(s, )( ,w)(w,a)(a,s)(s, )( ,i)(i,t)(t, )( ,m)(m,a)(a,t)(t,h)(h, )( ,c)(c,a)(a,p)(p,i)(i,t)(t,a)(a,t)(t,e)(e,d)(d, )( ,l)(l,i)(i,e)(e,v)(v,i)(i,n)(n,g)(g, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,p)(p,r)(r,o)(o,n)(n,a)(a,l)(l,i)(i,o)(o,n)(n,s)(s, )( ,o)(o,o)(o,d)(d, )( ,s)(s,e)(e,c)(c,r)(r,o)(o,b)(b,e)(e,d)(d, )( ,t)(t,o)(o, )( ,c)\n",
      "chars: mg communived s was it math capitated lieving of the pronalions ood secrobed to \n",
      "bigrams: (c,y)(y, )( ,j)(j,u)(u,r)(r,i)(i,c)(c,a)(a,l)(l, )( ,b)(b,u)(u,g)(g,s)(s, )( ,h)(h,o)(o,r)(r, )( ,c)(c,u)(u,r)(r,c)(c,e)(e,d)(d,i)(i,t)(t,i)(i,c)(c,a)(a,l)(l, )( ,b)(b,o)(o,m)(m,i)(i,n)(n,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e,i)(i,r)(r, )( ,o)(o,f)(f, )( ,v)(v,i)(i,g)(g,n)(n,i)(i,s)(s,h)(h, )( ,m)(m,i)(i,g)(g,a)(a,n)(n, )( ,i)(i,n)(n, )( ,a)(a,n)(n, )( ,k)(k,e)(e, )( ,t)(t,h)\n",
      "chars: cy jurical bugs hor curceditical bomination in their of vignish migan in an ke t\n",
      "bigrams: (i,b)(b,u)(u,t)(t, )( ,r)(r,e)(e,f)(f,e)(e,w)(w,o)(o,r)(r,e)(e,d)(d, )( ,a)(a,n)(n,d)(d, )( ,a)(a,s)(s, )( ,w)(w,h)(h,i)(i,c)(c,h)(h, )( ,e)(e,l)(l,e)(e,g)(g,r)(r,e)(e,r)(r, )( ,s)(s,e)(e,n)(n,t)(t, )( ,s)(s,o)(o,u)(u,n)(n,t)(t,e)(e,r)(r, )( ,t)(t,w)(w,o)(o, )( ,o)(o,r)(r,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,s)(s,i)(i,x)(x, )( ,o)(o,n)(n,e)(e, )\n",
      "chars: ibut refewored and as which elegrer sent sounter two ore nine three nine six one\n",
      "bigrams: (h,y)(y,m)(m,i)(i,s)(s,e)(e, )( ,o)(o,t)(t,s)(s, )( ,i)(i,t)(t, )( ,s)(s,p)(p,e)(e,a)(a,r)(r, )( ,p)(p,o)(o,p)(p,u)(u,l)(l,a)(a,r)(r,s)(s, )( ,p)(p,r)(r,o)(o,v)(v,i)(i,d)(d,u)(u,a)(a,l)(l, )( ,v)(v,e)(e,a)(a,n)(n, )( ,c)(c,h)(h,r)(r,o)(o,p)(p,l)(l, )( ,c)(c,a)(a,n)(n,d)(d, )( ,t)(t,h)(h,e)(e,i)(i,r)(r, )( ,c)(c,o)(o,n)(n,t)(t,e)(e,r)(r,v)(v,i)(i,s)(s,m)(m, )( ,e)(e,t)(t,a)(a,l)(l,i)(i,f)(f,i)(i,c)\n",
      "chars: hymise ots it spear populars providual vean chropl cand their contervism etalifi\n",
      "bigrams: (a,a)(a,p)(p,o)(o,s)(s, )( ,o)(o,r)(r, )( ,s)(s,o)(o,c)(c,i)(i,a)(a,l)(l, )( ,i)(i,n)(n,d)(d,e)(e,r)(r,m)(m,a)(a,n)(n,s)(s, )( ,c)(c,a)(a,s)(s,i)(i,o)(o,n)(n, )( ,o)(o,n)(n, )( ,c)(c,o)(o,n)(n,t)(t,r)(r,o)(o,n)(n,e)(e,r)(r, )( ,o)(o,r)(r, )( ,i)(i,s)(s, )( ,r)(r,o)(o,u)(u,n)(n,d)(d,e)(e,r)(r, )( ,t)(t,h)(h,e)(e, )( ,o)(o,n)(n,l)(l,y)(y, )( ,t)(t,h)(h,e)(e, )( ,n)(n,e)(e,w)(w, )( ,l)(l,i)(i,b)(b,s)\n",
      "chars: aapos or social indermans casion on controner or is rounder the only the new lib\n",
      "================================================================================\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 5100: 1.738439 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5200: 1.704535 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5300: 1.710657 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5400: 1.704913 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5500: 1.716912 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5600: 1.724354 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5700: 1.695396 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5800: 1.678772 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5900: 1.736940 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 6000: 1.727472 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "================================================================================\n",
      "bigrams: (i,x)(x, )( ,a)(a,l)(l,s)(s,o)(o, )( ,c)(c,o)(o,l)(l,o)(o,n)(n,i)(i,c)(c,a)(a,t)(t,i)(i,v)(v,e)(e,s)(s, )( ,w)(w,a)(a,s)(s, )( ,h)(h,a)(a,s)(s, )( ,e)(e,n)(n,d)(d,o)(o,m)(m,i)(i,c)(c,h)(h, )( ,a)(a,t)(t, )( ,i)(i,l)(l, )( ,c)(c,o)(o,n)(n,t)(t,r)(r,o)(o,l)(l,e)(e,c)(c,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,i)(i,s)(s, )( ,i)(i,t)(t, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)\n",
      "chars: ix also colonicatives was has endomich at il controlections is it one nine zero \n",
      "bigrams: (k, )( ,i)(i,s)(s, )( ,t)(t,h)(h,e)(e, )( ,p)(p,e)(e,r)(r,c)(c,e)(e,s)(s,s)(s, )( ,t)(t,r)(r,a)(a,y)(y, )( ,h)(h,a)(a,v)(v,e)(e, )( ,i)(i, )( ,n)(n,o)(o,b)(b,a)(a,d)(d,e)(e, )( ,d)(d,a)(a,y)(y, )( ,l)(l,e)(e,a)(a,d)(d,e)(e,s)(s, )( ,a)(a,n)(n,d)(d, )( ,c)(c,u)(u,l)(l,t)(t,r)(r,i)(i,c)(c, )( ,d)(d,a)(a,n)(n,t)(t,e)(e,r)(r, )( ,a)(a, )( ,s)(s,e)(e,r)(r,v)(v,e)(e,r)(r, )( ,i)(i,n)(n, )( ,i)(i,n)(n, )\n",
      "chars: k is the percess tray have i nobade day leades and cultric danter a server in in\n",
      "bigrams: (y,o)(o,s)(s,e)(e, )( ,a)(a,n)(n,d)(d, )( ,a)(a,r)(r,t)(t,s)(s, )( ,a)(a,s)(s, )( ,m)(m,o)(o,d)(d,e)(e,d)(d, )( ,c)(c,o)(o,m)(m,p)(p,a)(a,n)(n, )( ,f)(f,r)(r,e)(e,q)(q,u)(u,i)(i,n)(n,e)(e,s)(s, )( ,l)(l,i)(i,v)(v,e)(e,r)(r, )( ,a)(a, )( ,n)(n,o)(o,r)(r,m)(m,a)(a,n)(n,n)(n, )( ,e)(e,n)(n, )( ,m)(m,o)(o,s)(s,t)(t, )( ,l)(l,i)(i,f)(f,f)(f, )( ,c)(c,a)(a,n)(n,d)(d,i)(i,n)(n,s)(s, )( ,m)(m,a)(a,t)(t,e)\n",
      "chars: yose and arts as moded compan frequines liver a normann en most liff candins mat\n",
      "bigrams: (g,r)(r,a)(a,w)(w, )( ,o)(o,n)(n, )( ,h)(h,a)(a,v)(v,e)(e, )( ,a)(a,l)(l,l)(l, )( ,c)(c,h)(h,u)(u,r)(r,c)(c,t)(t, )( ,l)(l,i)(i,k)(k,e)(e, )( ,a)(a, )( ,l)(l,u)(u,t)(t,a)(a,l)(l, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,y)(y,e)(e,v)(v,e)(e,r)(r, )( ,o)(o,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,b)(b,o)(o,a)(a,n)(n, )( ,v)(v,i)(i,d)(d,e)(e, )( ,b)(b,e)(e,e)(e,n)(n, )( ,i)(i,s)(s, )( ,a)\n",
      "chars: graw on have all churct like a lutal one nine yever one eight boan vide been is \n",
      "bigrams: (i,h)(u,i)(i,c)(c,a)(a,t)(t,e)(e, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,e)(e,n)(n,g)(g,l)(l,e)(e,s)(s, )( ,i)(i,s)(s, )( ,n)(n,u)(u,d)(d,e)(e,s)(s, )( ,f)(f,o)(o,r)(r, )( ,c)(c,e)(e,n)(n,t)(t,r)(r,o)(o,d)(d, )( ,r)(r,a)(a,c)(c,a)(a,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,e)(e,x)(x,p)(p, )( ,h)(h,o)(o,n)(n, )( ,a)(a,l)(l,l)(l,o)(o,w)(w,e)(e,e)(e,r)(r, )( ,i)(i,n)(n, )( ,u)(u,s)(s,i)(i,v)(v,e)(e,t)(t,s)(s, )\n",
      "chars: iuicate of the engles is nudes for centrod racations exp hon alloweer in usivets\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6100: 1.711313 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 6200: 1.738704 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6300: 1.716272 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 6400: 1.720175 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 6500: 1.713369 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6600: 1.738632 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 6700: 1.740154 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 6800: 1.726610 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 6900: 1.714782 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 7000: 1.690737 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "================================================================================\n",
      "bigrams: (m,g)(i,s)(s,s)(s, )( ,n)(n,e)(e,w)(w, )( ,e)(e,n)(n,g)(g,i)(i,n)(n,e)(e,y)(y, )( ,b)(b,u)(u,t)(t, )( ,f)(f,a)(a,r)(r,c)(c,h)(h, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,t)(t,w)(w,o)(o, )( ,p)(p,a)(a,r)(r,t)(t,i)(i,n)(n,g)(g, )( ,o)(o,f)(f, )( ,b)(b,i)(i,l)(l,l)(l,a)(a,t)(t,e)(e, )( ,s)(s,e)(e,a)(a,r)(r,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,m)(m,e)(e,l)(l,i)(i,e)(e,v)(v,e)(e,d)(d, )\n",
      "chars: miss new enginey but farch three eight two parting of billate searation melieved\n",
      "bigrams: (i,u)(u,r)(r,i)(i,c)(c, )( ,c)(c,o)(o,m)(m,p)(p,o)(o,t)(t, )( ,t)(t,h)(h,e)(e, )( ,r)(r,e)(e,l)(l,a)(a,r)(r, )( ,h)(h,o)(o,l)(l,d)(d, )( ,a)(a,n)(n,d)(d, )( ,n)(n,o)(o,t)(t,t)(t,e)(e,r)(r, )( ,h)(h,e)(e,r)(r,e)(e, )( ,a)(a,s)(s, )( ,b)(b,e)(e, )( ,b)(b,e)(e, )( ,a)(a,r)(r,c)(c,t)(t,o)(o,r)(r,d)(d, )( ,a)(a, )( ,m)(m,i)(i,n)(n,s)(s,u)(u,r)(r,i)(i,a)(a, )( ,c)(c,o)(o,m)(m,m)(m,i)(i,f)(f,f)(f,i)(i,c)\n",
      "chars: iuric compot the relar hold and notter here as be be arctord a minsuria commiffi\n",
      "bigrams: (b,w)(w,a)(a,r)(r,e)(e, )( ,f)(f,o)(o,r)(r,t)(t,s)(s, )( ,i)(i,o)(o,e)(e,m)(m,o)(o,r)(r,e)(e,s)(s, )( ,p)(p,a)(a,r)(r,a)(a,c)(c,t)(t, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,p)(p,r)(r,o)(o,f)(f,e)(e,r)(r,s)(s, )( ,b)(b,a)(a,i)(i,l)(l,o)(o,w)(w,i)(i, )( ,a)(a,t)(t,i)(i,m)(m,i)(i,n)(n,g)(g, )( ,o)(o,u)(u,t)(t, )( ,w)(w,e)(e,r)(r,a)(a,b)(b,l)(l,e)(e, )( ,p)(p,l)(l,a)(a,y)(y,e)(e,d)(d,e)(e, )( ,o)(o,f)\n",
      "chars: bware forts ioemores paract in the profers bailowi atiming out werable playede o\n",
      "bigrams: (z,j)(y,l)(l,i)(i,c)(c,a)(a,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,h)(h,a)(a,d)(d, )( ,a)(a,u)(u,s)(s,u)(u,p)(p,e)(e,d)(d,i)(i,s)(s,m)(m, )( ,a)(a,n)(n,d)(d, )( ,a)(a,t)(t, )( ,t)(t,h)(h,a)(a,t)(t, )( ,b)(b,e)(e, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,p)(p,e)(e,r)(r,f)(f,u)(u,r)(r,y)(y, )( ,e)(e,t)(t,h)(h,e)(e,r)(r, )( ,o)(o,r)(r, )( ,s)(s,c)(c,i)(i,d)(d,e)(e,d)(d, )( ,a)(a,n)(n,d)(d, )( ,s)(s,o)\n",
      "chars: zylications had ausupedism and at that be one nine perfury ether or scided and s\n",
      "bigrams: (b,e)(e,r)(r, )( ,r)(r,o)(o,o)(o,t)(t, )( ,o)(o,f)(f, )( ,w)(w,a)(a,l)(l,l)(l, )( ,g)(g,a)(a,m)(m,e)(e,s)(s, )( ,w)(w,a)(a,d)(d,y)(y, )( ,s)(s,a)(a,r)(r,m)(m,a)(a,n)(n, )( ,i)(i, )( ,t)(t,h)(h,i)(i,s)(s, )( ,a)(a,n)(n, )( ,t)(t,h)(h,e)(e,m)(m, )( ,u)(u,s)(s,e)(e, )( ,a)(a,n)(n,d)(d, )( ,a)(a,f)(f,f)(f,e)(e,c)(c,i)(i,l)(l,e)(e,s)(s, )( ,a)(a,c)(c,t)(t,i)(i,o)(o,n)(n, )( ,o)(o,f)(f, )( ,t)(t,r)(r,i)\n",
      "chars: ber root of wall games wady sarman i this an them use and affeciles action of tr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.59\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "      batches = bigram_embed_train_batches.next()\n",
    "      feed_dict = dict()\n",
    "      for i in range(num_unrollings + 1):\n",
    "        feed_dict[train_data[i]] = batches[i]\n",
    "      _, l, predictions, lr = session.run(\n",
    "        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "      mean_loss += l\n",
    "      if step % summary_frequency == 0:\n",
    "        if step > 0:\n",
    "          mean_loss = mean_loss / summary_frequency\n",
    "        # The mean loss is an estimate of the loss over the last few batches.\n",
    "        print(\n",
    "          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "        mean_loss = 0\n",
    "        labels = np.concatenate(list(batches)[1:])\n",
    "        # convert to one-hot-encodings\n",
    "        noembed_labels = np.zeros(predictions.shape)\n",
    "        for i, j in enumerate(labels):\n",
    "            noembed_labels[i, j] = 1.0\n",
    "        print('Minibatch perplexity: %.2f' % float(\n",
    "          np.exp(logprob(predictions, noembed_labels))))\n",
    "        if step % (summary_frequency * 10) == 0:\n",
    "          # Generate some samples.\n",
    "          print('=' * 80)\n",
    "          for _ in range(5):\n",
    "            feed = bigram_sample(bigram_random_distribution())\n",
    "            bigram_sentence = bigram_characters(feed)[0]\n",
    "            sentence = bigram_first_characters(feed)[0]\n",
    "            # convert to embedding\n",
    "            feed = [np.argmax(feed)]\n",
    "            reset_sample_state.run()\n",
    "            for _ in range(79):\n",
    "              prediction = sample_prediction.eval({sample_input: feed})\n",
    "              feed = bigram_sample(prediction)\n",
    "              bigram_sentence += bigram_characters(feed)[0]\n",
    "              sentence += bigram_first_characters(feed)[0]\n",
    "              feed = [np.argmax(feed)]\n",
    "            print('bigrams:', bigram_sentence)\n",
    "            print('chars:', sentence)\n",
    "          print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "        reset_sample_state.run()\n",
    "        valid_logprob = 0\n",
    "        for _ in range(valid_size):\n",
    "          b = bigram_embed_valid_batches.next()\n",
    "          predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "          labels = np.zeros((1, bigram_size))\n",
    "          labels[0, b[1]] = 1.0\n",
    "          valid_logprob = valid_logprob + logprob(predictions, labels)\n",
    "        print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "          valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
